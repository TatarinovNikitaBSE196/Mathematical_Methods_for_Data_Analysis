{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2\n",
    "\n",
    "### Attention!\n",
    "* For tasks where <ins>text answer</ins> is required **Russian language** is **allowed**.\n",
    "* If a task asks you to describe something (make coclusions) then **text answer** is **mandatory** and **is** part of the task\n",
    "* We **only** accept **ipynb** notebooks. If you use Google Colab then you'll have to download the notebook before passing the homework\n",
    "* **Do not** use python loops instead of NumPy vector operations over NumPy vectors - it significantly decreases performance (see why https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/), will be punished with -0.25 for **every** task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Boston Dataset from sklearn (based on UCI ML housing dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston() # load dataset\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "columns = data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n",
      "(13,)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(columns.shape)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [0.5 points] \n",
    "Create Pandas DataFrame and split the data into train and test sets with ratio 80:20 with random_state=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 51.5 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X, columns=columns)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404 entries, 220 to 172\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     404 non-null    float64\n",
      " 1   ZN       404 non-null    float64\n",
      " 2   INDUS    404 non-null    float64\n",
      " 3   CHAS     404 non-null    float64\n",
      " 4   NOX      404 non-null    float64\n",
      " 5   RM       404 non-null    float64\n",
      " 6   AGE      404 non-null    float64\n",
      " 7   DIS      404 non-null    float64\n",
      " 8   RAD      404 non-null    float64\n",
      " 9   TAX      404 non-null    float64\n",
      " 10  PTRATIO  404 non-null    float64\n",
      " 11  B        404 non-null    float64\n",
      " 12  LSTAT    404 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 44.2 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 102 entries, 329 to 108\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     102 non-null    float64\n",
      " 1   ZN       102 non-null    float64\n",
      " 2   INDUS    102 non-null    float64\n",
      " 3   CHAS     102 non-null    float64\n",
      " 4   NOX      102 non-null    float64\n",
      " 5   RM       102 non-null    float64\n",
      " 6   AGE      102 non-null    float64\n",
      " 7   DIS      102 non-null    float64\n",
      " 8   RAD      102 non-null    float64\n",
      " 9   TAX      102 non-null    float64\n",
      " 10  PTRATIO  102 non-null    float64\n",
      " 11  B        102 non-null    float64\n",
      " 12  LSTAT    102 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 11.2 KB\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "X_train.info()\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. [1 point] \n",
    "Train models on train data using StatsModels( or sckit-learn) library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404 entries, 0 to 403\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     404 non-null    float64\n",
      " 1   ZN       404 non-null    float64\n",
      " 2   INDUS    404 non-null    float64\n",
      " 3   CHAS     404 non-null    float64\n",
      " 4   NOX      404 non-null    float64\n",
      " 5   RM       404 non-null    float64\n",
      " 6   AGE      404 non-null    float64\n",
      " 7   DIS      404 non-null    float64\n",
      " 8   RAD      404 non-null    float64\n",
      " 9   TAX      404 non-null    float64\n",
      " 10  PTRATIO  404 non-null    float64\n",
      " 11  B        404 non-null    float64\n",
      " 12  LSTAT    404 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 41.2 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102 entries, 0 to 101\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     102 non-null    float64\n",
      " 1   ZN       102 non-null    float64\n",
      " 2   INDUS    102 non-null    float64\n",
      " 3   CHAS     102 non-null    float64\n",
      " 4   NOX      102 non-null    float64\n",
      " 5   RM       102 non-null    float64\n",
      " 6   AGE      102 non-null    float64\n",
      " 7   DIS      102 non-null    float64\n",
      " 8   RAD      102 non-null    float64\n",
      " 9   TAX      102 non-null    float64\n",
      " 10  PTRATIO  102 non-null    float64\n",
      " 11  B        102 non-null    float64\n",
      " 12  LSTAT    102 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 10.5 KB\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=columns)\n",
    "X_train_scaled.info()\n",
    "X_test_scaled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression: test RMSE = 5.7835\n",
      "Linear regression: test R^2 = 0.5892\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "linear_regression_model = sm.OLS(y_train, X_train_scaled)\n",
    "\n",
    "linear_regression_results = linear_regression_model.fit()\n",
    "linear_regression_y_pred = linear_regression_results.predict(X_test_scaled)\n",
    "\n",
    "print(\"Linear regression: test RMSE = %.4f\" % sqrt(mean_squared_error(y_test, linear_regression_y_pred)))\n",
    "print(\"Linear regression: test R^2 = %.4f\" % r2_score(y_test, linear_regression_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: test RMSE = 5.8270\n",
      "Ridge: test R^2 = 0.5830\n"
     ]
    }
   ],
   "source": [
    "ridge_results = linear_regression_model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "ridge_y_pred = ridge_results.predict(X_test_scaled)\n",
    "\n",
    "print(\"Ridge: test RMSE = %.4f\" % sqrt(mean_squared_error(y_test, ridge_y_pred)))\n",
    "print(\"Ridge: test R^2 = %.4f\" % r2_score(y_test, ridge_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso: test RMSE = 5.7962\n",
      "Lasso: test R^2 = 0.5874\n"
     ]
    }
   ],
   "source": [
    "lasso_results = linear_regression_model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "lasso_y_pred = lasso_results.predict(X_test_scaled)\n",
    "\n",
    "print(\"Lasso: test RMSE = %.4f\" % sqrt(mean_squared_error(y_test, lasso_y_pred)))\n",
    "print(\"Lasso: test R^2 = %.4f\" % r2_score(y_test, lasso_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. [1 point] \n",
    "Explore the values of the parameters of the resulting models and compare the number of zero weights in them. \n",
    "\n",
    "Comment on the significance of the coefficients, overal model significance and other related factors from the results table. \n",
    "\n",
    "`Hint` Use StatModels to obtain significance of the coefficients. They ca be found on the `summary` of the fitted linear model. \n",
    "It might be tricky to obtain `summary` for the regularized model. Please, read the documentation of the StatModels library to figure out how to do that, e.g.   [OLSResults](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults) class might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>     <td>0.765</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>2370.9385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-10-11 23:45</td>        <td>BIC:</td>         <td>2426.9583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>404</td>         <td>Log-Likelihood:</td>    <td>-1171.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>13</td>           <td>F-statistic:</td>       <td>102.2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>390</td>       <td>Prob (F-statistic):</td> <td>9.64e-117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.773</td>            <td>Scale:</td>         <td>20.020</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>      <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>22.6119</td>  <td>0.2226</td>  <td>101.5764</td> <td>0.0000</td> <td>22.1742</td> <td>23.0495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>-0.9708</td>  <td>0.2980</td>   <td>-3.2575</td> <td>0.0012</td> <td>-1.5568</td> <td>-0.3849</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>1.0571</td>   <td>0.3408</td>   <td>3.1022</td>  <td>0.0021</td> <td>0.3872</td>  <td>1.7271</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>0.0383</td>   <td>0.4428</td>   <td>0.0865</td>  <td>0.9311</td> <td>-0.8324</td> <td>0.9090</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>0.5945</td>   <td>0.2291</td>   <td>2.5946</td>  <td>0.0098</td> <td>0.1440</td>  <td>1.0450</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>-1.8551</td>  <td>0.4846</td>   <td>-3.8282</td> <td>0.0002</td> <td>-2.8079</td> <td>-0.9024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>2.5732</td>   <td>0.3175</td>   <td>8.1058</td>  <td>0.0000</td> <td>1.9491</td>  <td>3.1974</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>-0.0876</td>  <td>0.4022</td>   <td>-0.2178</td> <td>0.8277</td> <td>-0.8784</td> <td>0.7032</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>-2.8809</td>  <td>0.4446</td>   <td>-6.4800</td> <td>0.0000</td> <td>-3.7550</td> <td>-2.0068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>2.1122</td>   <td>0.6069</td>   <td>3.4805</td>  <td>0.0006</td> <td>0.9191</td>  <td>3.3054</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>-1.8753</td>  <td>0.6652</td>   <td>-2.8191</td> <td>0.0051</td> <td>-3.1832</td> <td>-0.5675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>-2.2928</td>  <td>0.3003</td>   <td>-7.6359</td> <td>0.0000</td> <td>-2.8831</td> <td>-1.7024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>0.7182</td>   <td>0.2613</td>   <td>2.7486</td>  <td>0.0063</td> <td>0.2045</td>  <td>1.2319</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>-3.5925</td>  <td>0.3954</td>   <td>-9.0855</td> <td>0.0000</td> <td>-4.3698</td> <td>-2.8151</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>141.494</td>  <td>Durbin-Watson:</td>    <td>1.996</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>629.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>      <td>1.470</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>8.365</td>   <td>Condition No.:</td>     <td>10</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Results: Ordinary least squares\n",
       "==================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.765    \n",
       "Dependent Variable: y                AIC:                2370.9385\n",
       "Date:               2021-10-11 23:45 BIC:                2426.9583\n",
       "No. Observations:   404              Log-Likelihood:     -1171.5  \n",
       "Df Model:           13               F-statistic:        102.2    \n",
       "Df Residuals:       390              Prob (F-statistic): 9.64e-117\n",
       "R-squared:          0.773            Scale:              20.020   \n",
       "--------------------------------------------------------------------\n",
       "           Coef.    Std.Err.      t       P>|t|     [0.025    0.975]\n",
       "--------------------------------------------------------------------\n",
       "const     22.6119     0.2226   101.5764   0.0000   22.1742   23.0495\n",
       "CRIM      -0.9708     0.2980    -3.2575   0.0012   -1.5568   -0.3849\n",
       "ZN         1.0571     0.3408     3.1022   0.0021    0.3872    1.7271\n",
       "INDUS      0.0383     0.4428     0.0865   0.9311   -0.8324    0.9090\n",
       "CHAS       0.5945     0.2291     2.5946   0.0098    0.1440    1.0450\n",
       "NOX       -1.8551     0.4846    -3.8282   0.0002   -2.8079   -0.9024\n",
       "RM         2.5732     0.3175     8.1058   0.0000    1.9491    3.1974\n",
       "AGE       -0.0876     0.4022    -0.2178   0.8277   -0.8784    0.7032\n",
       "DIS       -2.8809     0.4446    -6.4800   0.0000   -3.7550   -2.0068\n",
       "RAD        2.1122     0.6069     3.4805   0.0006    0.9191    3.3054\n",
       "TAX       -1.8753     0.6652    -2.8191   0.0051   -3.1832   -0.5675\n",
       "PTRATIO   -2.2928     0.3003    -7.6359   0.0000   -2.8831   -1.7024\n",
       "B          0.7182     0.2613     2.7486   0.0063    0.2045    1.2319\n",
       "LSTAT     -3.5925     0.3954    -9.0855   0.0000   -4.3698   -2.8151\n",
       "------------------------------------------------------------------\n",
       "Omnibus:             141.494       Durbin-Watson:          1.996  \n",
       "Prob(Omnibus):       0.000         Jarque-Bera (JB):       629.882\n",
       "Skew:                1.470         Prob(JB):               0.000  \n",
       "Kurtosis:            8.365         Condition No.:          10     \n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>     <td>0.765</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>2372.3540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-10-11 23:45</td>        <td>BIC:</td>         <td>2428.3738</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>404</td>         <td>Log-Likelihood:</td>    <td>-1172.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>13</td>           <td>F-statistic:</td>       <td>101.7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>390</td>       <td>Prob (F-statistic):</td> <td>1.90e-116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.772</td>            <td>Scale:</td>         <td>20.091</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>      <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>22.3880</td>  <td>0.2230</td>  <td>100.3946</td> <td>0.0000</td> <td>21.9496</td> <td>22.8264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>-0.9389</td>  <td>0.2986</td>   <td>-3.1449</td> <td>0.0018</td> <td>-1.5259</td> <td>-0.3519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>0.9965</td>   <td>0.3414</td>   <td>2.9190</td>  <td>0.0037</td> <td>0.3253</td>  <td>1.6676</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>-0.0576</td>  <td>0.4436</td>   <td>-0.1298</td> <td>0.8968</td> <td>-0.9298</td> <td>0.8146</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>0.6098</td>   <td>0.2295</td>   <td>2.6566</td>  <td>0.0082</td> <td>0.1585</td>  <td>1.0611</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>-1.7222</td>  <td>0.4854</td>   <td>-3.5477</td> <td>0.0004</td> <td>-2.6766</td> <td>-0.7678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>2.6120</td>   <td>0.3180</td>   <td>8.2135</td>  <td>0.0000</td> <td>1.9868</td>  <td>3.2372</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>-0.1155</td>  <td>0.4029</td>   <td>-0.2867</td> <td>0.7745</td> <td>-0.9078</td> <td>0.6767</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>-2.7539</td>  <td>0.4454</td>   <td>-6.1834</td> <td>0.0000</td> <td>-3.6295</td> <td>-1.8783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>1.8348</td>   <td>0.6079</td>   <td>3.0181</td>  <td>0.0027</td> <td>0.6396</td>  <td>3.0301</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>-1.6265</td>  <td>0.6664</td>   <td>-2.4407</td> <td>0.0151</td> <td>-2.9366</td> <td>-0.3163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>-2.2561</td>  <td>0.3008</td>   <td>-7.5007</td> <td>0.0000</td> <td>-2.8475</td> <td>-1.6648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>0.7188</td>   <td>0.2617</td>   <td>2.7463</td>  <td>0.0063</td> <td>0.2042</td>  <td>1.2334</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>-3.5287</td>  <td>0.3961</td>   <td>-8.9086</td> <td>0.0000</td> <td>-4.3074</td> <td>-2.7499</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>147.660</td>  <td>Durbin-Watson:</td>    <td>1.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>694.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>      <td>1.521</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>8.659</td>   <td>Condition No.:</td>     <td>10</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Results: Ordinary least squares\n",
       "==================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.765    \n",
       "Dependent Variable: y                AIC:                2372.3540\n",
       "Date:               2021-10-11 23:45 BIC:                2428.3738\n",
       "No. Observations:   404              Log-Likelihood:     -1172.2  \n",
       "Df Model:           13               F-statistic:        101.7    \n",
       "Df Residuals:       390              Prob (F-statistic): 1.90e-116\n",
       "R-squared:          0.772            Scale:              20.091   \n",
       "--------------------------------------------------------------------\n",
       "           Coef.    Std.Err.      t       P>|t|     [0.025    0.975]\n",
       "--------------------------------------------------------------------\n",
       "const     22.3880     0.2230   100.3946   0.0000   21.9496   22.8264\n",
       "CRIM      -0.9389     0.2986    -3.1449   0.0018   -1.5259   -0.3519\n",
       "ZN         0.9965     0.3414     2.9190   0.0037    0.3253    1.6676\n",
       "INDUS     -0.0576     0.4436    -0.1298   0.8968   -0.9298    0.8146\n",
       "CHAS       0.6098     0.2295     2.6566   0.0082    0.1585    1.0611\n",
       "NOX       -1.7222     0.4854    -3.5477   0.0004   -2.6766   -0.7678\n",
       "RM         2.6120     0.3180     8.2135   0.0000    1.9868    3.2372\n",
       "AGE       -0.1155     0.4029    -0.2867   0.7745   -0.9078    0.6767\n",
       "DIS       -2.7539     0.4454    -6.1834   0.0000   -3.6295   -1.8783\n",
       "RAD        1.8348     0.6079     3.0181   0.0027    0.6396    3.0301\n",
       "TAX       -1.6265     0.6664    -2.4407   0.0151   -2.9366   -0.3163\n",
       "PTRATIO   -2.2561     0.3008    -7.5007   0.0000   -2.8475   -1.6648\n",
       "B          0.7188     0.2617     2.7463   0.0063    0.2042    1.2334\n",
       "LSTAT     -3.5287     0.3961    -8.9086   0.0000   -4.3074   -2.7499\n",
       "------------------------------------------------------------------\n",
       "Omnibus:             147.660       Durbin-Watson:          1.990  \n",
       "Prob(Omnibus):       0.000         Jarque-Bera (JB):       694.793\n",
       "Skew:                1.521         Prob(JB):               0.000  \n",
       "Kurtosis:            8.659         Condition No.:          10     \n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLSResults(linear_regression_model, ridge_results.params, linear_regression_model.normalized_cov_params).summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>     <td>0.765</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>2371.0439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-10-11 23:45</td>        <td>BIC:</td>         <td>2427.0637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>404</td>         <td>Log-Likelihood:</td>    <td>-1171.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>13</td>           <td>F-statistic:</td>       <td>102.1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>390</td>       <td>Prob (F-statistic):</td> <td>1.01e-116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.773</td>            <td>Scale:</td>         <td>20.025</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>      <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>22.6019</td>  <td>0.2226</td>  <td>101.5182</td> <td>0.0000</td> <td>22.1642</td> <td>23.0396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>-0.9403</td>  <td>0.2981</td>   <td>-3.1547</td> <td>0.0017</td> <td>-1.5263</td> <td>-0.3543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>1.0215</td>   <td>0.3408</td>   <td>2.9972</td>  <td>0.0029</td> <td>0.3514</td>  <td>1.6916</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>0.0000</td>   <td>0.4429</td>   <td>0.0000</td>  <td>1.0000</td> <td>-0.8708</td> <td>0.8708</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>0.5948</td>   <td>0.2292</td>   <td>2.5955</td>  <td>0.0098</td> <td>0.1442</td>  <td>1.0453</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>-1.8029</td>  <td>0.4847</td>   <td>-3.7200</td> <td>0.0002</td> <td>-2.7558</td> <td>-0.8500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>2.5852</td>   <td>0.3175</td>   <td>8.1423</td>  <td>0.0000</td> <td>1.9609</td>  <td>3.2094</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>-0.0690</td>  <td>0.4023</td>   <td>-0.1715</td> <td>0.8639</td> <td>-0.8599</td> <td>0.7220</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>-2.8085</td>  <td>0.4446</td>   <td>-6.3162</td> <td>0.0000</td> <td>-3.6827</td> <td>-1.9343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>1.9567</td>   <td>0.6070</td>   <td>3.2238</td>  <td>0.0014</td> <td>0.7634</td>  <td>3.1501</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>-1.7392</td>  <td>0.6653</td>   <td>-2.6141</td> <td>0.0093</td> <td>-3.0472</td> <td>-0.4311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>-2.2788</td>  <td>0.3003</td>   <td>-7.5884</td> <td>0.0000</td> <td>-2.8692</td> <td>-1.6884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>0.7056</td>   <td>0.2613</td>   <td>2.7000</td>  <td>0.0072</td> <td>0.1918</td>  <td>1.2193</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>-3.5969</td>  <td>0.3955</td>   <td>-9.0956</td> <td>0.0000</td> <td>-4.3744</td> <td>-2.8194</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>143.585</td>  <td>Durbin-Watson:</td>    <td>1.997</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>649.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>      <td>1.488</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>8.455</td>   <td>Condition No.:</td>     <td>10</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Results: Ordinary least squares\n",
       "==================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.765    \n",
       "Dependent Variable: y                AIC:                2371.0439\n",
       "Date:               2021-10-11 23:45 BIC:                2427.0637\n",
       "No. Observations:   404              Log-Likelihood:     -1171.5  \n",
       "Df Model:           13               F-statistic:        102.1    \n",
       "Df Residuals:       390              Prob (F-statistic): 1.01e-116\n",
       "R-squared:          0.773            Scale:              20.025   \n",
       "--------------------------------------------------------------------\n",
       "           Coef.    Std.Err.      t       P>|t|     [0.025    0.975]\n",
       "--------------------------------------------------------------------\n",
       "const     22.6019     0.2226   101.5182   0.0000   22.1642   23.0396\n",
       "CRIM      -0.9403     0.2981    -3.1547   0.0017   -1.5263   -0.3543\n",
       "ZN         1.0215     0.3408     2.9972   0.0029    0.3514    1.6916\n",
       "INDUS      0.0000     0.4429     0.0000   1.0000   -0.8708    0.8708\n",
       "CHAS       0.5948     0.2292     2.5955   0.0098    0.1442    1.0453\n",
       "NOX       -1.8029     0.4847    -3.7200   0.0002   -2.7558   -0.8500\n",
       "RM         2.5852     0.3175     8.1423   0.0000    1.9609    3.2094\n",
       "AGE       -0.0690     0.4023    -0.1715   0.8639   -0.8599    0.7220\n",
       "DIS       -2.8085     0.4446    -6.3162   0.0000   -3.6827   -1.9343\n",
       "RAD        1.9567     0.6070     3.2238   0.0014    0.7634    3.1501\n",
       "TAX       -1.7392     0.6653    -2.6141   0.0093   -3.0472   -0.4311\n",
       "PTRATIO   -2.2788     0.3003    -7.5884   0.0000   -2.8692   -1.6884\n",
       "B          0.7056     0.2613     2.7000   0.0072    0.1918    1.2193\n",
       "LSTAT     -3.5969     0.3955    -9.0956   0.0000   -4.3744   -2.8194\n",
       "------------------------------------------------------------------\n",
       "Omnibus:             143.585       Durbin-Watson:          1.997  \n",
       "Prob(Omnibus):       0.000         Jarque-Bera (JB):       649.888\n",
       "Skew:                1.488         Prob(JB):               0.000  \n",
       "Kurtosis:            8.455         Condition No.:          10     \n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLSResults(linear_regression_model, lasso_results.params, linear_regression_model.normalized_cov_params).summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Модели показали схожие результаты, причём качество всех моделей достаточно высокое: (R^2 ~ 0.773, F-statistic > 100). Незначимыми оказались параметры INDUS (P > 0.89) и AGE (P > 0.77).```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4. [1 point] \n",
    "Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions. \n",
    "It's enough to apply to one of the models above (e.g simple linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(df, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df, y, test_size=test_size, random_state=0)\n",
    "    X_train_scaled = sm.add_constant(pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns))\n",
    "    X_test_scaled = sm.add_constant(pd.DataFrame(scaler.transform(X_test), columns=df.columns))\n",
    "    return mean_squared_error(y_test, sm.OLS(y_train, X_train_scaled).fit().predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_mse(df, y, test_size):\n",
    "    mse_array = np.zeros(dtype=np.float64, shape=(len(df.columns)))\n",
    "    for index in range(0, len(df.columns)):\n",
    "        mse_array[index] = get_mse(df.drop(df.columns[index], axis=1), y, test_size)\n",
    "    return np.argsort(mse_array)[0], np.min(mse_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(df, y, test_size):\n",
    "    cur_mse = get_mse(df, y, test_size)\n",
    "    while len(df.columns) > 0:\n",
    "        index_of_worst, worst_mse = get_worst_mse(df, y, test_size)\n",
    "        if worst_mse >= cur_mse:\n",
    "            return cur_mse, df\n",
    "        df = df.drop(df.columns[index_of_worst], axis=1)\n",
    "        cur_mse = worst_mse\n",
    "    return cur_mse, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial RMSE: 5.78350931508514\n",
      "New RMSE: 5.780222921822188\n",
      "['AGE']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>     <td>0.765</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>2370.9385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-10-11 23:46</td>        <td>BIC:</td>         <td>2426.9583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>404</td>         <td>Log-Likelihood:</td>    <td>-1171.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>13</td>           <td>F-statistic:</td>       <td>102.2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>390</td>       <td>Prob (F-statistic):</td> <td>9.64e-117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.773</td>            <td>Scale:</td>         <td>20.020</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>      <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>22.6119</td>  <td>0.2226</td>  <td>101.5764</td> <td>0.0000</td> <td>22.1742</td> <td>23.0495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>-0.9708</td>  <td>0.2980</td>   <td>-3.2575</td> <td>0.0012</td> <td>-1.5568</td> <td>-0.3849</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>1.0571</td>   <td>0.3408</td>   <td>3.1022</td>  <td>0.0021</td> <td>0.3872</td>  <td>1.7271</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>0.0383</td>   <td>0.4428</td>   <td>0.0865</td>  <td>0.9311</td> <td>-0.8324</td> <td>0.9090</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>0.5945</td>   <td>0.2291</td>   <td>2.5946</td>  <td>0.0098</td> <td>0.1440</td>  <td>1.0450</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>-1.8551</td>  <td>0.4846</td>   <td>-3.8282</td> <td>0.0002</td> <td>-2.8079</td> <td>-0.9024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>2.5732</td>   <td>0.3175</td>   <td>8.1058</td>  <td>0.0000</td> <td>1.9491</td>  <td>3.1974</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>-0.0876</td>  <td>0.4022</td>   <td>-0.2178</td> <td>0.8277</td> <td>-0.8784</td> <td>0.7032</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>-2.8809</td>  <td>0.4446</td>   <td>-6.4800</td> <td>0.0000</td> <td>-3.7550</td> <td>-2.0068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>2.1122</td>   <td>0.6069</td>   <td>3.4805</td>  <td>0.0006</td> <td>0.9191</td>  <td>3.3054</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>-1.8753</td>  <td>0.6652</td>   <td>-2.8191</td> <td>0.0051</td> <td>-3.1832</td> <td>-0.5675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>-2.2928</td>  <td>0.3003</td>   <td>-7.6359</td> <td>0.0000</td> <td>-2.8831</td> <td>-1.7024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>0.7182</td>   <td>0.2613</td>   <td>2.7486</td>  <td>0.0063</td> <td>0.2045</td>  <td>1.2319</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>-3.5925</td>  <td>0.3954</td>   <td>-9.0855</td> <td>0.0000</td> <td>-4.3698</td> <td>-2.8151</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>141.494</td>  <td>Durbin-Watson:</td>    <td>1.996</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>629.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>      <td>1.470</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>8.365</td>   <td>Condition No.:</td>     <td>10</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Results: Ordinary least squares\n",
       "==================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.765    \n",
       "Dependent Variable: y                AIC:                2370.9385\n",
       "Date:               2021-10-11 23:46 BIC:                2426.9583\n",
       "No. Observations:   404              Log-Likelihood:     -1171.5  \n",
       "Df Model:           13               F-statistic:        102.2    \n",
       "Df Residuals:       390              Prob (F-statistic): 9.64e-117\n",
       "R-squared:          0.773            Scale:              20.020   \n",
       "--------------------------------------------------------------------\n",
       "           Coef.    Std.Err.      t       P>|t|     [0.025    0.975]\n",
       "--------------------------------------------------------------------\n",
       "const     22.6119     0.2226   101.5764   0.0000   22.1742   23.0495\n",
       "CRIM      -0.9708     0.2980    -3.2575   0.0012   -1.5568   -0.3849\n",
       "ZN         1.0571     0.3408     3.1022   0.0021    0.3872    1.7271\n",
       "INDUS      0.0383     0.4428     0.0865   0.9311   -0.8324    0.9090\n",
       "CHAS       0.5945     0.2291     2.5946   0.0098    0.1440    1.0450\n",
       "NOX       -1.8551     0.4846    -3.8282   0.0002   -2.8079   -0.9024\n",
       "RM         2.5732     0.3175     8.1058   0.0000    1.9491    3.1974\n",
       "AGE       -0.0876     0.4022    -0.2178   0.8277   -0.8784    0.7032\n",
       "DIS       -2.8809     0.4446    -6.4800   0.0000   -3.7550   -2.0068\n",
       "RAD        2.1122     0.6069     3.4805   0.0006    0.9191    3.3054\n",
       "TAX       -1.8753     0.6652    -2.8191   0.0051   -3.1832   -0.5675\n",
       "PTRATIO   -2.2928     0.3003    -7.6359   0.0000   -2.8831   -1.7024\n",
       "B          0.7182     0.2613     2.7486   0.0063    0.2045    1.2319\n",
       "LSTAT     -3.5925     0.3954    -9.0855   0.0000   -4.3698   -2.8151\n",
       "------------------------------------------------------------------\n",
       "Omnibus:             141.494       Durbin-Watson:          1.996  \n",
       "Prob(Omnibus):       0.000         Jarque-Bera (JB):       629.882\n",
       "Skew:                1.470         Prob(JB):               0.000  \n",
       "Kurtosis:            8.365         Condition No.:          10     \n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Initial RMSE:', sqrt(get_mse(df, y, 0.2)))\n",
    "new_mse, new_df = forward(df, y, 0.2)\n",
    "print('New RMSE:', sqrt(new_mse))\n",
    "print([item for item in df.columns.to_list() if item not in new_df.columns.to_list()])\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "X_train_scaled = sm.add_constant(pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns))\n",
    "X_test_scaled = sm.add_constant(pd.DataFrame(scaler.transform(X_test), columns=df.columns))\n",
    "sm.OLS(y_train, X_train_scaled).fit().summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Как мы видим, в результате Forward Elimination все значимые параметры остались в модели и только 1 из незначимых параметров был исключён из рассмотрение, причём более незначимый (в соответствии с P) INDUS остался.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 5. [1 point] \n",
    "Find the best (in terms of RMSE) $\\alpha$ for Ridge regression using cross-validation with 5 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 5.7248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CV score')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "X_train_scaled = sm.add_constant(pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns))\n",
    "X_test_scaled = sm.add_constant(pd.DataFrame(scaler.transform(X_test), columns=df.columns))\n",
    "\n",
    "alphas = np.logspace(-4, 3, 1000)\n",
    "searcher = GridSearchCV(Ridge(), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=5)\n",
    "searcher.fit(X_train_scaled, y_train)\n",
    "print(\"Best alpha = %.4f\" % searcher.best_params_[\"alpha\"])\n",
    "\n",
    "plt.plot(alphas, -searcher.cv_results_[\"mean_test_score\"])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"CV score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] \n",
    "**Implement a linear regression model for the MSE loss function, trained by gradient descent.**\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Euclidean norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} q_{i_{k}} (w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "Exponentially weighed averages can provide a better estimate which is closer to the actual gradient.\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a zero or random (from a normal distribution) vector. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum', \n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum'\n",
    "        delta: float\n",
    "            proportion of object in a batch (fot stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.delta = delta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        if self.gd_type not in ['GradientDescent', 'StochasticDescent', 'Momentum']:\n",
    "            raise Exception('Unknown type')\n",
    "        np.random.seed(0)\n",
    "        self.loss_history = []\n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.zeros(X.shape[1])\n",
    "        self.w = np.array(self.w0)\n",
    "        cur_w = np.array(self.w)\n",
    "        h = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(0, self.max_iter):\n",
    "            if self.gd_type == 'GradientDescent':\n",
    "                self.w -= self.eta * self.calc_gradient(X, y)\n",
    "            elif self.gd_type == 'StochasticDescent':\n",
    "                indexes = np.random.choice(X.shape[0], int(X.shape[0] * self.delta))\n",
    "                self.w -= self.eta * self.calc_gradient(np.take(X, indexes, axis=0), np.take(y, indexes))\n",
    "            else:\n",
    "                indexes = np.random.choice(X.shape[0], int(X.shape[0] * self.delta))\n",
    "                h = self.alpha * h + self.eta * self.calc_gradient(np.take(X, indexes, axis=0), np.take(y, indexes))\n",
    "                self.w -= h\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "            if np.linalg.norm(self.w - cur_w) < self.tolerance:\n",
    "                break\n",
    "            cur_w = np.array(self.w)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return np.dot(X, self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * np.dot(X.T, np.dot(X, self.w) - y) / y.shape[0]\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        return np.mean((self.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. [1 points] \n",
    "Train and validate \"hand-written\" model (simple linear regression) on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 5.7860\n"
     ]
    }
   ],
   "source": [
    "momentum_model = LinReg(gd_type='Momentum', max_iter=5000).fit(X_train_scaled, y_train)\n",
    "print(\"Test RMSE = %.4f\" % sqrt(mean_squared_error(y_test, momentum_model.predict(X_test_scaled))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_types = np.array(['GradientDescent', 'StochasticDescent', 'Momentum'])\n",
    "max_iters = np.arange(50, 5000, 200)\n",
    "alphas = np.logspace(-4, 3, 20)\n",
    "searcher = GridSearchCV(LinReg(), [{\"gd_type\": gd_types, \"max_iter\": max_iters, \"alpha\": alphas}], \n",
    "                        scoring=\"neg_mean_squared_error\", cv=5)\n",
    "searcher.fit(X_train_scaled, y_train)\n",
    "print(\"Best gd_type = %s\" % searcher.best_params_[\"gd_type\"])\n",
    "print(\"Best max_iter = %d\" % searcher.best_params_[\"max_iter\"])\n",
    "print(\"Best alpha = %.4f\" % searcher.best_params_[\"alpha\"])\n",
    "print(\"Test RMSE = %.4f\" % sqrt(mean_squared_error(y_test, LinReg(gd_type='', max_iter=, alpha=).fit(X_train_scaled, y_train).predict(X_test_scaled))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Модель LinReg для Momentum имеет немного большую ошибку, чем встроенные модели из задачи 1.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. [1 points] \n",
    "Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD and Momentum. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```your conclusions here```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
